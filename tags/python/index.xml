<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Ka Ian Chan</title>
    <link>https://kaian9804.github.io/tags/python/</link>
    <description>Recent content in python on Ka Ian Chan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Oct 2023 00:42:00 +0800</lastBuildDate><atom:link href="https://kaian9804.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CNN &#43; Transfer Learning</title>
      <link>https://kaian9804.github.io/post/05_transfer_learning/</link>
      <pubDate>Thu, 12 Oct 2023 00:42:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/05_transfer_learning/</guid>
      <description>The Working Concepts of Transfer Learning Use previous trained leverage knowledge (e.g. features, weights) to train new models
Tackle problems like having less data for the newer task
The more related the tasks is, the better the metrics of the hybrid model
Generally, inputs are fed to the base model
Usually omit the last layer when the base model features / weights are transferred
The embeddings from the penultimate layer of the base model are then transferred to another model which extracts different parts according to its needs</description>
    </item>
    
    <item>
      <title>Concolutional Neural Networks (CNNs)</title>
      <link>https://kaian9804.github.io/post/04_cnn/</link>
      <pubDate>Wed, 11 Oct 2023 18:22:08 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/04_cnn/</guid>
      <description>Architecture Convolution Layer: Can separates and identifies the various features of images (= feature extraction)
Pooling Layer: Summarise the features presents in a region of the feature map generated by a convoution layer (= reduce dimensions)
Fully-Connected Layer: Use the output from the convolution process, and predicts the class of the image based on the features extracted in previous stages. It can only handle one-dimensions layer.
Other information Input shape should represent data reshaped as an image</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory (LSTM)</title>
      <link>https://kaian9804.github.io/post/03_lstm/</link>
      <pubDate>Wed, 11 Oct 2023 10:03:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/03_lstm/</guid>
      <description>Long Short-Term Memory (LSTM) Architecture LSTM uses gates to deal with calcualtions. It is used with time series data, but tabular is not time series data.
Forget Gate: Will forget the useless information (no longer needed)
Learn Gate: Combine the Event (= current input) and STM together, so the recently learnt necessary information (from STM) can apply to the current input
Remember Gate: Have not forget the LTM information, STM and Event are combined in remember gate which work as updated LTM</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory (LSTM)</title>
      <link>https://kaian9804.github.io/post/999_template/</link>
      <pubDate>Wed, 11 Oct 2023 10:03:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/999_template/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Coding for Multi-layer Perceptron (MLP)</title>
      <link>https://kaian9804.github.io/post/02_mlp/</link>
      <pubDate>Tue, 10 Oct 2023 00:42:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/02_mlp/</guid>
      <description>1. Loading the Library from numpy import loadtxtfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import BatchNormalization, Dense 2. Loading Dataset df = loadtxt(&amp;#34;dataset.csv&amp;#34;, delimiter=&amp;#39;,&amp;#39;) 3. Spliting Features and label For example, the data includes 5 features and 1 label.
x = df[:, 0:5]y = df[:, 5] 4. Define Neural Network Architecture model = Sequential()model.add(Dense(12, input_dim=5, activation=&amp;#39;relu&amp;#39;))model.add(BatchNormalization())model.add(Dense(8, activation=&amp;#39;relu&amp;#39;))model.add(BatchNormalization())model.add(Dense(1, activation=&amp;#39;sigmoid&amp;#39;)) 5. Compile the Model model.compile(loss = &amp;#39;binary_crossentropy&amp;#39;, # this is a binary problemoptimizer = &amp;#39;adam&amp;#39;,metrics = [&amp;#39;accuracy&amp;#39;]) 6.</description>
    </item>
    
    <item>
      <title>Feature Engineering with Stepwise Regression</title>
      <link>https://kaian9804.github.io/post/01_stepwise_regression/</link>
      <pubDate>Sun, 08 Oct 2023 21:16:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/01_stepwise_regression/</guid>
      <description>When I was studying statistics, I noticed an algorithm called &amp;ldquo;stepwise regression&amp;rdquo;. Generally, this is a method that ranks features based on their importances and finds out how important each feature is to the prediction. The following coding will show the process of finding the features importances. (Assumed that this is a binary classification) Import the tools import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import LabelEncoder from</description>
    </item>
    
  </channel>
</rss>

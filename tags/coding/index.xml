<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>coding on Ka Ian Chan</title>
    <link>https://kaian9804.github.io/tags/coding/</link>
    <description>Recent content in coding on Ka Ian Chan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Oct 2023 10:03:00 +0800</lastBuildDate><atom:link href="https://kaian9804.github.io/tags/coding/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Long Short-Term Memory (LSTM)</title>
      <link>https://kaian9804.github.io/post/03_lstm/</link>
      <pubDate>Wed, 11 Oct 2023 10:03:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/03_lstm/</guid>
      <description>Long Short-Term Memory (LSTM) Architecture LSTM uses gates to deal with calcualtions. It is used with time series data, but tabular is not time series data.
Forget Gate: Will forget the useless information (no longer needed)
Learn Gate: Combine the Event (= current input) and STM together, so the recently learnt necessary information (from STM) can apply to the current input
Remember Gate: Have not forget the LTM information, STM and Event are combined in remember gate which work as updated LTM</description>
    </item>
    
    <item>
      <title>Coding for Multi-layer Perceptron (MLP)</title>
      <link>https://kaian9804.github.io/post/02_mlp/</link>
      <pubDate>Tue, 10 Oct 2023 00:42:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/02_mlp/</guid>
      <description>1. Loading the Library from numpy import loadtxtfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import BatchNormalization, Dense 2. Loading Dataset df = loadtxt(&amp;#34;dataset.csv&amp;#34;, delimiter=&amp;#39;,&amp;#39;) 3. Spliting Features and label For example, the data includes 5 features and 1 label.
x = df[:, 0:5]y = df[:, 5] 4. Define Neural Network Architecture model = Sequential()model.add(Dense(12, input_dim=5, activation=&amp;#39;relu&amp;#39;))model.add(BatchNormalization())model.add(Dense(8, activation=&amp;#39;relu&amp;#39;))model.add(BatchNormalization())model.add(Dense(1, activation=&amp;#39;sigmoid&amp;#39;)) 5. Compile the Model model.compile(loss = &amp;#39;binary_crossentropy&amp;#39;, # this is a binary problemoptimizer = &amp;#39;adam&amp;#39;,metrics = [&amp;#39;accuracy&amp;#39;]) 6.</description>
    </item>
    
    <item>
      <title>Feature Engineering with Stepwise Regression</title>
      <link>https://kaian9804.github.io/post/01_stepwise_regression/</link>
      <pubDate>Sun, 08 Oct 2023 21:16:00 +0800</pubDate>
      
      <guid>https://kaian9804.github.io/post/01_stepwise_regression/</guid>
      <description>When I was studying statistics, I noticed an algorithm called &amp;ldquo;stepwise regression&amp;rdquo;. Generally, this is a method that ranks features based on their importances and finds out how important each feature is to the prediction. The following coding will show the process of finding the features importances. (Assumed that this is a binary classification) Import the tools import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import LabelEncoder from</description>
    </item>
    
  </channel>
</rss>
